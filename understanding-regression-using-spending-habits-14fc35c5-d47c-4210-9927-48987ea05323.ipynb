{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7768117,"sourceType":"datasetVersion","datasetId":4544119}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"name":"Understanding Regression Using Spending Habits"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"\n# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n# THEN FEEL FREE TO DELETE THIS CELL.\n# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n# NOTEBOOK.\n\nimport os\nimport sys\nfrom tempfile import NamedTemporaryFile\nfrom urllib.request import urlopen\nfrom urllib.parse import unquote, urlparse\nfrom urllib.error import HTTPError\nfrom zipfile import ZipFile\nimport tarfile\nimport shutil\n\nCHUNK_SIZE = 40960\nDATA_SOURCE_MAPPING = 'student-spending-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4544119%2F7768117%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240316%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240316T153110Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D3bc56e4c8cde67dd752b5fa2bd6326488b4424e47430afa06d2f9b6dfdf04e6d5f87fa219f3295ba87ce22e189d2807b547f5a5487b400359dab8ba35934ddbe4026bd3ed03dfcd9e956e7940f8c5a5eee1d30febde902ec4d6a83c1b64eb436bc32b7845dc67d79d0782d058293eea315fd9e4b42063e87e4eaade556a6a7e1111ef75e5624343ca2aaaddde8e1276e9ffe29cd4078d84e43eac9b5e9b37b3a2073fef7d645aea0a84b584af45006c9f812f5d27520b4d334504980b8053a8c020ca09daefb9d7b66272b9f2e983ae7e1cc24466e1fb71022b7cf547621571e93194d2e300bc33c2ffa32525adee52600796615e03ead770ce8da7ffa57c45a'\n\nKAGGLE_INPUT_PATH='/kaggle/input'\nKAGGLE_WORKING_PATH='/kaggle/working'\nKAGGLE_SYMLINK='kaggle'\n\n!umount /kaggle/input/ 2> /dev/null\nshutil.rmtree('/kaggle/input', ignore_errors=True)\nos.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\nos.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n\ntry:\n  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\nexcept FileExistsError:\n  pass\ntry:\n  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\nexcept FileExistsError:\n  pass\n\nfor data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n    directory, download_url_encoded = data_source_mapping.split(':')\n    download_url = unquote(download_url_encoded)\n    filename = urlparse(download_url).path\n    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n    try:\n        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n            total_length = fileres.headers['content-length']\n            print(f'Downloading {directory}, {total_length} bytes compressed')\n            dl = 0\n            data = fileres.read(CHUNK_SIZE)\n            while len(data) > 0:\n                dl += len(data)\n                tfile.write(data)\n                done = int(50 * dl / int(total_length))\n                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n                sys.stdout.flush()\n                data = fileres.read(CHUNK_SIZE)\n            if filename.endswith('.zip'):\n              with ZipFile(tfile) as zfile:\n                zfile.extractall(destination_path)\n            else:\n              with tarfile.open(tfile.name) as tarfile:\n                tarfile.extractall(destination_path)\n            print(f'\\nDownloaded and uncompressed: {directory}')\n    except HTTPError as e:\n        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n        continue\n    except OSError as e:\n        print(f'Failed to load {download_url} to path {destination_path}')\n        continue\n\nprint('Data source import complete.')\n","metadata":{},"cell_type":"code","outputs":[],"execution_count":0},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-14T18:27:58.11404Z","iopub.execute_input":"2024-03-14T18:27:58.114586Z","iopub.status.idle":"2024-03-14T18:27:58.122325Z","shell.execute_reply.started":"2024-03-14T18:27:58.114553Z","shell.execute_reply":"2024-03-14T18:27:58.121062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Disclaimer\n* I am someone who is still fairly new to the field of data science and machine learning so much of my code and practices may not be ideal. But I do hope that this notebook can provide you with a good general understanding of using machine learning for regression.\n\n* If you are more experienced than me and notice problems, please feel free to let me know.\n\n* Most of the information in this notebook on building the model is from the course \"Supervised Machine Learning Regression Classification\" by Andrew Ng on Coursera. Please check out that course if you're interested, the explainations in that course were honestly amazing.","metadata":{}},{"cell_type":"markdown","source":"# Objective\nThis will be a simple practice lab of implementing multivariate regression in order to predict the spending habits of a given student.\n\nThe data given by the data set is purely fictional, therefore this notebook should only be used for practicing data analysis techniques and should not be used for any actual application.","metadata":{}},{"cell_type":"markdown","source":"# Exploratory Data Analysis\nHere we will view and explore the data to get a better understanding of what features we can include in our regression.","metadata":{}},{"cell_type":"code","source":"# viewing the data\nspending = pd.read_csv('/kaggle/input/student-spending-dataset/student_spending (1).csv')\n\nprint('First Few rows')\nprint(spending.head())\nprint('\\n')\nprint('Info')\nprint(spending.info())","metadata":{"execution":{"iopub.status.busy":"2024-03-14T18:27:58.124796Z","iopub.execute_input":"2024-03-14T18:27:58.12524Z","iopub.status.idle":"2024-03-14T18:27:58.191734Z","shell.execute_reply.started":"2024-03-14T18:27:58.125204Z","shell.execute_reply":"2024-03-14T18:27:58.190524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# looking at missing values\nprint(spending.isna().sum())","metadata":{"execution":{"iopub.status.busy":"2024-03-14T18:27:58.193624Z","iopub.execute_input":"2024-03-14T18:27:58.194047Z","iopub.status.idle":"2024-03-14T18:27:58.202177Z","shell.execute_reply.started":"2024-03-14T18:27:58.194008Z","shell.execute_reply":"2024-03-14T18:27:58.201001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Wowie no filling na needs to be done!! But we don't need this unnamed column since by the looks of it, it's just another index column and may get in the way of things later.","metadata":{}},{"cell_type":"code","source":"spending.drop(labels = 'Unnamed: 0', axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T18:27:58.205041Z","iopub.execute_input":"2024-03-14T18:27:58.205429Z","iopub.status.idle":"2024-03-14T18:27:58.214242Z","shell.execute_reply.started":"2024-03-14T18:27:58.2054Z","shell.execute_reply":"2024-03-14T18:27:58.213026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now lets break down the spending into graphs that are easier to interpret and then compare it across groups\nHow can we do this?\n1. Create a new pandas series with the mean for all students, this will just essentially be our baseline\n2. Then we create more series with the mean for students of a group that we are filtering by i.e. Major, Year, Gender, Age\n3. Lastly we compare across groups","metadata":{}},{"cell_type":"code","source":"# the actual things that students spend money on\nspendings = ['tuition', 'housing', 'food', 'transportation', 'books_supplies', 'entertainment', 'personal_care', 'technology', 'health_wellness', 'miscellaneous']","metadata":{"execution":{"iopub.status.busy":"2024-03-14T18:27:58.215754Z","iopub.execute_input":"2024-03-14T18:27:58.216131Z","iopub.status.idle":"2024-03-14T18:27:58.224527Z","shell.execute_reply.started":"2024-03-14T18:27:58.216102Z","shell.execute_reply":"2024-03-14T18:27:58.223492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating series for spendings\ntotal_mean = spending[spendings]\ntotal_mean = total_mean.mean()\n\nprint(total_mean)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T18:27:58.225773Z","iopub.execute_input":"2024-03-14T18:27:58.226077Z","iopub.status.idle":"2024-03-14T18:27:58.239888Z","shell.execute_reply.started":"2024-03-14T18:27:58.226052Z","shell.execute_reply":"2024-03-14T18:27:58.23875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categories = total_mean.index\nheights = total_mean.values\n\nbottom = 0\nfor category, height in zip(categories, heights):\n    plt.bar('Total Spending', height, bottom=bottom, label=category)\n    bottom += height\n    \nplt.ylabel('Spending')\nplt.title('Stacked Bar Chart of Mean Spending')\nplt.legend()\n\nprint('Mean Spending for a Student is ', total_mean.sum())","metadata":{"execution":{"iopub.status.busy":"2024-03-14T18:27:58.241061Z","iopub.execute_input":"2024-03-14T18:27:58.241906Z","iopub.status.idle":"2024-03-14T18:27:58.768208Z","shell.execute_reply.started":"2024-03-14T18:27:58.241875Z","shell.execute_reply":"2024-03-14T18:27:58.767013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We see that the vast majority of this spending is consumed by the tuition. Now lets view the spending seperated by the major of the individual.","metadata":{}},{"cell_type":"code","source":"total_spend = total_mean.sum()\nprint(total_spend)\n\nmajor_mean = spending[spending['major'] == 'Psychology']\nmajor_mean = major_mean[spendings]\nmajor_mean = major_mean.mean()\ncomp_sci_spend = major_mean.sum()\nprint(comp_sci_spend)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T18:27:58.769475Z","iopub.execute_input":"2024-03-14T18:27:58.769857Z","iopub.status.idle":"2024-03-14T18:27:58.781354Z","shell.execute_reply.started":"2024-03-14T18:27:58.769803Z","shell.execute_reply":"2024-03-14T18:27:58.780165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"majors = spending['major'].unique()\nprint(majors)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T18:27:58.782668Z","iopub.execute_input":"2024-03-14T18:27:58.783015Z","iopub.status.idle":"2024-03-14T18:27:58.791471Z","shell.execute_reply.started":"2024-03-14T18:27:58.782988Z","shell.execute_reply":"2024-03-14T18:27:58.790404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mean spending by major\n# for ax, major in zip(axes.flatten(), majors):\n#     major_mean = spending[spending['major'] == major]\n#     # it's probably not best practice to have a variable named spending and another named spendings\n#     major_mean = major_mean[spendings]\n#     major_mean = major_mean.mean()\n#     categories = major_mean.index\n#     heights = major_mean.values\n\n#     bottom = 0\n#     for category, height in zip(categories, heights):\n#         ax.bar('Total Spending', height, bottom=bottom, label=category)\n#         bottom += height\n\n#     ax.set_ylabel('Spending')\n#     ax.set_title(f'Mean Spending for {major} Majors')\n#     ax.legend()\n\n# ^ code for plotting stacked bar charts, not working too well and also stacked bar charts not\n# necessary as this is mostly for comparing the difference in total spending habits rather than\n# by category\n\n\nby_majors = spending.groupby('major')[spendings].mean().reset_index() #return a dataframe with the mean spending of each major\n\n# making bar charts\ncategory = by_majors['major'].values\nheights = by_majors[spendings].sum(axis = 1).values\n\nplt.bar(category, heights)\nplt.ylabel('Spending')\nplt.ylim(6000, 6500)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T18:27:58.795292Z","iopub.execute_input":"2024-03-14T18:27:58.79576Z","iopub.status.idle":"2024-03-14T18:27:59.049167Z","shell.execute_reply.started":"2024-03-14T18:27:58.795711Z","shell.execute_reply":"2024-03-14T18:27:59.047986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\nThere seems to be some difference in the spendings by majors but not much. This could be because the dataset used is fictional and thus does not take into account the different factors that can influence the spendings of a student.\n\nBut lets Try this again but with different features such as age, gender, monthly income, financial aid, and preferred payment method","metadata":{}},{"cell_type":"code","source":"# spending by gender\ngenders = spending['gender'].unique()\n\nby_genders = spending.groupby('gender')[spendings].mean().reset_index()\n\n# making bar charts\ncategory = by_genders['gender'].values\nheights = by_genders[spendings].sum(axis = 1).values\n\nplt.bar(category, heights)\nplt.ylabel('Spending')\nplt.ylim(6000,6500)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T18:27:59.050337Z","iopub.execute_input":"2024-03-14T18:27:59.050619Z","iopub.status.idle":"2024-03-14T18:27:59.288872Z","shell.execute_reply.started":"2024-03-14T18:27:59.050594Z","shell.execute_reply":"2024-03-14T18:27:59.287568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again there is little to no difference","metadata":{}},{"cell_type":"code","source":"# spending by preferred payment method\npref_pay_method = spending['preferred_payment_method'].unique()\n\nby_pref_pay_method = spending.groupby('preferred_payment_method')[spendings].mean().reset_index()\n\n# making bar charts\ncategory = by_pref_pay_method['preferred_payment_method'].values\nheights = by_pref_pay_method[spendings].sum(axis = 1).values\n\nplt.bar(category, heights)\nplt.ylabel('Spending')\nplt.ylim(6000,6500)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T18:27:59.290378Z","iopub.execute_input":"2024-03-14T18:27:59.291116Z","iopub.status.idle":"2024-03-14T18:27:59.52703Z","shell.execute_reply.started":"2024-03-14T18:27:59.291084Z","shell.execute_reply":"2024-03-14T18:27:59.525892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Still very minor differences","metadata":{}},{"cell_type":"code","source":"# spending by age\nage = spending['age'].unique()\n\nby_age = spending.groupby('age')[spendings].mean().reset_index()\n\n# making bar charts\ncategory = by_age['age'].values\nheights = by_age[spendings].sum(axis = 1).values\n\nplt.bar(category, heights, width = 0.8)\nplt.ylabel('Spending')\nplt.ylim(6000,6500)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T18:27:59.528467Z","iopub.execute_input":"2024-03-14T18:27:59.528894Z","iopub.status.idle":"2024-03-14T18:27:59.825322Z","shell.execute_reply.started":"2024-03-14T18:27:59.528864Z","shell.execute_reply":"2024-03-14T18:27:59.822874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wowie there appears to be some sort of correlation between spending and age but keep in mind that this is a very zoomed in view of the graph.","metadata":{}},{"cell_type":"code","source":"# spending by monthly_income\nby_monthly_inc = spending['monthly_income'].reset_index()\nby_spendings = spending[spendings].reset_index()\nby_monthly_inc['spending'] = by_spendings.sum(axis = 1)\n\nplt.scatter(by_monthly_inc['monthly_income'], by_monthly_inc['spending'])\nplt.ylabel('Spending')\nplt.xlabel('Monthly Income')","metadata":{"execution":{"iopub.status.busy":"2024-03-14T18:27:59.826511Z","iopub.execute_input":"2024-03-14T18:27:59.826802Z","iopub.status.idle":"2024-03-14T18:28:00.140415Z","shell.execute_reply.started":"2024-03-14T18:27:59.826776Z","shell.execute_reply":"2024-03-14T18:28:00.139534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Little to no correlation between spending and monthly income","metadata":{}},{"cell_type":"code","source":"# spending by financial_aid\nby_fin_aid = spending['financial_aid'].reset_index()\nby_spendings = spending[spendings].reset_index()\nby_fin_aid['spending'] = by_spendings.sum(axis = 1)\n\nplt.scatter(by_fin_aid['financial_aid'], by_fin_aid['spending'])\nplt.ylabel('Spending')\nplt.xlabel('Financial Aid')","metadata":{"execution":{"iopub.status.busy":"2024-03-14T18:28:00.141487Z","iopub.execute_input":"2024-03-14T18:28:00.142302Z","iopub.status.idle":"2024-03-14T18:28:00.436609Z","shell.execute_reply.started":"2024-03-14T18:28:00.142272Z","shell.execute_reply":"2024-03-14T18:28:00.435181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Little Correlation between spending and financial aid","metadata":{}},{"cell_type":"markdown","source":"### Splitting Data into Training and Test Set\n**Purpose of Each Set:**\nTraining: Meant to train the model and obtain optimized parameters\nTesting: Taking the final model with the optimized parameters and testing it. Meant to be a simulation of a real life application of the data.\n\nSince Our Data is relatively small (1000 observations) we will have relatively large cross validation and test sets with respect to the training set. So we will use 70% for testing and 30% for testing. These numbers are fairly arbitrary so they can be modified depending on preferences.","metadata":{}},{"cell_type":"code","source":"# should have done this a lot earlier to make the visualization simpler ehe\n# don't be like me, do this when cleaning your data\nspending['Spending'] = spending[spendings].sum(axis = 1)\nprint(spending.head())","metadata":{"execution":{"iopub.status.busy":"2024-03-14T18:28:00.438439Z","iopub.execute_input":"2024-03-14T18:28:00.439027Z","iopub.status.idle":"2024-03-14T18:28:00.452003Z","shell.execute_reply.started":"2024-03-14T18:28:00.438996Z","shell.execute_reply":"2024-03-14T18:28:00.450945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Regression Model will take in the features of\n* age\n* gender\n* major\n* payment method\n\nAs these are the only features that had some sort of correlation with the total spending of the student. Try playing around with the features and seeing how the model does.","metadata":{}},{"cell_type":"code","source":"features = ['age', 'gender', 'major', 'preferred_payment_method']\n# we also have to clean up the spendings dataframe a bit since some of the features are categorical\n# turn categorical variables into dummy variables","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:33:46.386857Z","iopub.execute_input":"2024-03-14T19:33:46.387255Z","iopub.status.idle":"2024-03-14T19:33:46.392496Z","shell.execute_reply.started":"2024-03-14T19:33:46.387224Z","shell.execute_reply":"2024-03-14T19:33:46.391139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for feature in features[1:]:\n    print(f'Unique number of categories in {feature}: {spending[feature].unique()}')","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:40:13.241584Z","iopub.execute_input":"2024-03-14T19:40:13.241973Z","iopub.status.idle":"2024-03-14T19:40:13.248352Z","shell.execute_reply.started":"2024-03-14T19:40:13.241943Z","shell.execute_reply":"2024-03-14T19:40:13.247257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dummy_df1 = pd.get_dummies(spending['gender'])\ndummy_df2 = pd.get_dummies(spending['major'])\ndummy_df3 = pd.get_dummies(spending['preferred_payment_method'])\n\ndummy_df = pd.concat([dummy_df1, dummy_df2, dummy_df3], axis=1)\ndummy_df['age'] = spending['age']\nprint(dummy_df.head())","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:46:21.647175Z","iopub.execute_input":"2024-03-14T19:46:21.647541Z","iopub.status.idle":"2024-03-14T19:46:21.666138Z","shell.execute_reply.started":"2024-03-14T19:46:21.647512Z","shell.execute_reply":"2024-03-14T19:46:21.664889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# splitting data\nX_train = dummy_df.iloc[:700].values\nY_train = spending.iloc[:700]['Spending'].values\n\nX_test = dummy_df.iloc[700:1000].values\nY_test = spending.iloc[700:1000]['Spending'].values\n\nprint(X_train.shape)\nprint(Y_train.shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:47:06.025339Z","iopub.execute_input":"2024-03-14T19:47:06.025698Z","iopub.status.idle":"2024-03-14T19:47:06.035429Z","shell.execute_reply.started":"2024-03-14T19:47:06.025669Z","shell.execute_reply":"2024-03-14T19:47:06.034303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building the Regression Model","metadata":{}},{"cell_type":"markdown","source":"\\*Note: the following code takes a lot of code and equations from the labs provided in the Machine Learning Specialization by Andrew Ng on Coursera.","metadata":{}},{"cell_type":"markdown","source":"## How Linear Regression Works\n\n1. General Equation for the predicted value in univariate linear regression. Multivariate regression essentially works in the same way but with more weightings and more variables x.\n    $$f_{w,b}(x) = wx + b$$\n    \n\n2. Compute the cost. For now we are just going to use mean square error because it is simple and it gets the job done.\n    - Cost Function:\n\n    $$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{3}$$ \nwhere:\n$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{4} $$ \n\n3. Use Gradient Descent to find the optimal parameters for w and b given your dataset\n      - Gradient Descent:\n      $$b := b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}$$\n      $$w := w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w}$$\n      Where $\\alpha$ is the learning rate and $\\frac{\\partial J(w,b)}{\\partial variable}$ is the partial derivative of the cost function with respect to one of the parameters\n      \n\\* more will be explained about Cost and Gradient Descent Later","metadata":{}},{"cell_type":"markdown","source":"### Cost Function\n\nThe cost function can be thought of as a measure of how close the prediction made by the model given the parameters **w** and **b** are to the actual output. As **w** and **b** change with gradient descent, the loss associated with the parameters for **w** and **b** will also change.","metadata":{}},{"cell_type":"code","source":"def compute_cost(X, Y, w, b):\n    \"\"\"\n    X: numpy array: Shape (m,n)   Input Data\n    Y: numpy array: Shape (m,)    Expected Output\n    w: numpy array: Shape (n,)    Weightings\n    b: scalar:  Bias\n    \"\"\"\n    \n    m = X.shape[0]\n    cost = 0.0\n    for i in range(m):                                \n        f_wb_i = np.dot(X[i], w) + b\n        cost = cost + (f_wb_i - Y[i])**2       \n    cost = cost / (2 * m)                      \n    return cost\n\n# the code below provides a more detailed idea of what is going on but runs a lot slower since it does not utilize vectorization with np.dot()\n\n# m, n = X.shape\n#     total_cost = 0\n#     cost_sum = 0\n#     for i in range(m):\n#         f_wb = 0\n#         for j in range(n):\n#             f_wb_j = w[j] * X[i][j]\n#             f_wb += f_wb_j\n#         f_wb += b\n#         cost_i = (f_wb - y[i])**2\n#         cost_sum = cost_sum + cost_i\n        \n#     total_cost = cost_sum / (2*m)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:47:57.646357Z","iopub.execute_input":"2024-03-14T19:47:57.646724Z","iopub.status.idle":"2024-03-14T19:47:57.65472Z","shell.execute_reply.started":"2024-03-14T19:47:57.646696Z","shell.execute_reply":"2024-03-14T19:47:57.653826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Gradient Descent\nThe general formula for gradient descent in univariate linear regression is:\n$$b := b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}$$\n$$w := w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w}$$\n\nwhere, parameters **w** and **b** are both updated simultaniously and where  \n$$\n\\begin{align}\n\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{6}  \\\\\n\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{7}\n\\end{align}\n$$\n","metadata":{}},{"cell_type":"code","source":"def compute_gradient(X, Y, w, b): \n    \"\"\"\n    X: numpy array: Shape (m,n)   Input Data\n    Y: numpy array: Shape (m,)    Expected Output\n    w: numpy array: Shape (n,)    Weightings\n    b: scalar:  Bias\n     \"\"\"\n    m,n = X.shape\n    dj_dw = np.zeros((n,))\n    dj_db = 0.\n\n    for i in range(m):                             \n        err = (np.dot(X[i], w) + b) - Y[i]   \n        for j in range(n):                         \n            dj_dw[j] = dj_dw[j] + err * X[i, j]    \n        dj_db = dj_db + err                        \n    dj_dw = dj_dw / m                                \n    dj_db = dj_db / m                                \n    \n    return dj_db, dj_dw","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:48:00.781014Z","iopub.execute_input":"2024-03-14T19:48:00.781383Z","iopub.status.idle":"2024-03-14T19:48:00.791646Z","shell.execute_reply.started":"2024-03-14T19:48:00.781355Z","shell.execute_reply":"2024-03-14T19:48:00.790629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\ndef gradient_descent(X, Y, w, b, cost_function, gradient_function, alpha, num_iters): \n    \"\"\"\n    X: numpy array: Shape (m,n)   Input Data\n    y: numpy array: Shape (m,)    Expected Output\n    w: numpy array: Shape (n,)    Weightings\n    b: float                      Bias\n    cost_function:                function to compute cost made earlier\n    gradient_function:            function to compute the gradient\n    alpha: (float):               Learning rate\n    num_iters: (int):             number of iterations to run gradient descent\n      \"\"\"    \n    for i in range(num_iters):\n\n        # Computing the gradient \n        dj_db,dj_dw = gradient_function(X, Y, w, b)\n\n        # Update Parameters using w, b, alpha and gradient\n        w = w - alpha * dj_dw  \n        b = b - alpha * dj_db \n        \n        # Print cost every at intervals 10 times or as many iterations if < 10\n        if i% math.ceil(num_iters / 10) == 0:\n            J = cost_function(X, Y, w, b)\n            print(f\"Iteration {i:4d}: Cost {J:8.2f}   \")\n        \n    return w, b # returning the optimal w and b parameters found using the model","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:48:44.992417Z","iopub.execute_input":"2024-03-14T19:48:44.992781Z","iopub.status.idle":"2024-03-14T19:48:45.000749Z","shell.execute_reply.started":"2024-03-14T19:48:44.992753Z","shell.execute_reply":"2024-03-14T19:48:44.999884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing\nThe fun stuff!!!!","metadata":{}},{"cell_type":"code","source":"print(X_train)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:47:29.377753Z","iopub.execute_input":"2024-03-14T19:47:29.378142Z","iopub.status.idle":"2024-03-14T19:47:29.383521Z","shell.execute_reply.started":"2024-03-14T19:47:29.378113Z","shell.execute_reply":"2024-03-14T19:47:29.382356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training the model\nw = np.random.randn(12,) # the reason it is 12 is because there are 12 features in x train\nb = np.random.randn()\nalpha = 0.003 # found using trial and error\nnum_iters = 30000\n\nw_opt, b_opt = gradient_descent(X_train, Y_train, w, b, compute_cost, compute_gradient, alpha, num_iters)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:19:07.532832Z","iopub.execute_input":"2024-03-14T20:19:07.533234Z","iopub.status.idle":"2024-03-14T20:25:13.753616Z","shell.execute_reply.started":"2024-03-14T20:19:07.533203Z","shell.execute_reply":"2024-03-14T20:25:13.752689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# testing the model:\nm = X_test.shape[0]\nfor i in range(10):\n    rand_index = np.random.randint(m)\n    prediction = np.dot(X_test[i], w_opt) + b_opt\n    actual = Y_test[i]\n    print(f\"prediction: {prediction:0.2f}, target value: {actual}, discrepency: {prediction - actual:0.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:49:25.188037Z","iopub.execute_input":"2024-03-14T20:49:25.188858Z","iopub.status.idle":"2024-03-14T20:49:25.205677Z","shell.execute_reply.started":"2024-03-14T20:49:25.188777Z","shell.execute_reply":"2024-03-14T20:49:25.204198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusions\nOk so it seems that our model isn't really able to predict the actual value of the spending, this can be potentially due to many reason:\n- Overfitting: By running too many iterations, our model may be memorizing the training data rather than being able to generalize from the training data to make predictions.\n- Inadequate features: perhaps there were other features that we could have trained the model on but didn't due to not going in depth enough during the exploratory phase of data analysis\n- Nature of the data: given that the data is fictional, there could be a possibility that there is little correlation between the features and the actual data.","metadata":{}}]}